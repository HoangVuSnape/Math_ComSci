{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWf_xlggrK63"
      },
      "source": [
        "# Naive Bayes from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KJFJZHnNrK64"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from random import shuffle\n",
        "from math import exp, log\n",
        "from collections import defaultdict, Counter\n",
        "from typing import NamedTuple, List, Set, Tuple, Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "sOLn_3nIx7sT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GvXwAOZ5rK65"
      },
      "outputs": [],
      "source": [
        "# Ensure that we have a `data` directory we use to store downloaded data\n",
        "!mkdir -p data\n",
        "data_dir: Path = Path('data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "v_b1LkUJrK66",
        "outputId": "8a63a3dd-2214-4ccb-e97c-350179101214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘data/enron1.tar.gz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We're using the \"Enron Spam\" data set\n",
        "!wget -nc -P data http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.chdir(\"/content/data/enron1\")\n",
        "\n",
        "# Read and print summary.txt (assuming it's in the current directory)\n",
        "try:\n",
        "  with open(\"Summary.txt\", \"r\") as f:\n",
        "    print(f.read())\n",
        "except FileNotFoundError:\n",
        "  print(\"Summary.txt not found in data/enron1 directory\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPWyri-Du1S8",
        "outputId": "3b248e9f-ab2b-4128-a423-db287848b087"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Legitimate\n",
            "----------\n",
            "- Owner: farmer-d\n",
            "- Total number: 3672 emails\n",
            "- Date of first email: 1999-12-10\n",
            "- Date of last email: 2002-01-11\n",
            "- Similars deletion: No\n",
            "- Encoding: No\n",
            "\n",
            "\n",
            "Spam\n",
            "----\n",
            "- Owner: GP\n",
            "- Total number: 1500 emails\n",
            "- Date of first email: 2003-12-18\n",
            "- Date of last email: 2005-09-06\n",
            "- Similars deletion: No\n",
            "- Encoding: No\n",
            "\n",
            "Spam:Legitimate rate = 1:3\n",
            "Total number of emails (legitimate + spam): 5975\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "E_6J77rzrK66"
      },
      "outputs": [],
      "source": [
        "!tar -xzf data/enron1.tar.gz -C data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View Data"
      ],
      "metadata": {
        "id": "rUq8Hq-qA-vr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BvsQrKcnrK66"
      },
      "outputs": [],
      "source": [
        "# The data set has 2 directories: One for `spam` messages, one for `ham` messages\n",
        "spam_data_path: Path = data_dir / 'enron1' / 'spam'\n",
        "ham_data_path: Path = data_dir / 'enron1' / 'ham'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DXcdoLDXrK67"
      },
      "outputs": [],
      "source": [
        "# Our data container for `spam` and `ham` messages\n",
        "class Message(NamedTuple):\n",
        "    text: str\n",
        "    is_spam: bool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FK9W6gyKrK67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4574cd2f-d2aa-46a5-8b28-4b5f79dd960d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data/enron1/spam/1503.2004-07-07.GP.spam.txt',\n",
              " 'data/enron1/spam/1879.2004-08-18.GP.spam.txt',\n",
              " 'data/enron1/spam/3990.2005-03-07.GP.spam.txt',\n",
              " 'data/enron1/spam/1417.2004-06-25.GP.spam.txt',\n",
              " 'data/enron1/spam/4111.2005-03-20.GP.spam.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Globbing for all the `.txt` files in both (`spam` and `ham`) directories\n",
        "spam_message_paths: List[str] = glob.glob(str(spam_data_path / '*.txt'))\n",
        "ham_message_paths: List[str] = glob.glob(str(ham_data_path / '*.txt'))\n",
        "\n",
        "message_paths: List[str] = spam_message_paths + ham_message_paths\n",
        "message_paths[:5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(message_paths)"
      ],
      "metadata": {
        "id": "RgzMGOONvpKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3538f327-c4a9-4172-cee7-2f900691d531"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5172"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xYG3wvyerK68"
      },
      "outputs": [],
      "source": [
        "# The list which eventually contains all the parsed Enron `spam` and `ham` messages\n",
        "messages: List[Message] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "V-UPiR8xrK69"
      },
      "outputs": [],
      "source": [
        "# Open every file individually, turn it into a `Message` and append it to our `messages` list\n",
        "for path in message_paths:\n",
        "    with open(path, errors='ignore') as file:\n",
        "        is_spam: bool = True if 'spam' in path else False\n",
        "        # We're only interested in the subject for the time being\n",
        "        text: str = file.readline().replace('Subject:', '').strip()\n",
        "        messages.append(Message(text, is_spam))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3XIypq0ArK69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34dd73a8-a4ce-4802-c3cd-837263bbe565"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Message(text='hpl nom for may 16 , 2001', is_spam=False),\n",
              " Message(text='performance review - mid - year', is_spam=False),\n",
              " Message(text='re : chevron - winter', is_spam=False),\n",
              " Message(text='lst rev feb . 2000 josey ranch nom', is_spam=False),\n",
              " Message(text=\"she ' ll enjoy this .\", is_spam=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "shuffle(messages)\n",
        "messages[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-x_IN32-rK6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a903019-ea98-42f2-d4d7-568050d90bbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5172"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "iFSpbagIrK6-"
      },
      "outputs": [],
      "source": [
        "# Given a string, normalize and extract all words with length greater than 2\n",
        "def tokenize(text: str) -> Set[str]:\n",
        "    words: List[str] = []\n",
        "    for word in re.findall(r'[A-Za-z0-9\\']+', text):\n",
        "        if len(word) >= 2:\n",
        "            words.append(word.lower())\n",
        "    return set(words)\n",
        "\n",
        "assert tokenize('Is this a text? If so, Tokenize this text!...') == {'is', 'this', 'text', 'if', 'so', 'tokenize'}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert tokenize('Is this a text? If so, Tokenize this text!...') == {'is', 'this', 'text', 'if', 'so', 'tokenize', 'is'}"
      ],
      "metadata": {
        "id": "LcL_7YFUysMr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HzP3my2rrK6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa56f83-f738-422c-f152-be7aa885ca3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'16', '2001', 'for', 'hpl', 'may', 'nom'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "tokenize(messages[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cJxt6gGprK6-"
      },
      "outputs": [],
      "source": [
        "# Split the list of messages into a `train` and `test` set (defaults to 80/20 train/test split)\n",
        "def train_test_split(messages: List[Message], pct=0.8) -> Tuple[List[Message], List[Message]]:\n",
        "    shuffle(messages)\n",
        "    num_train = int(round(len(messages) * pct, 0))\n",
        "    return messages[:num_train], messages[num_train:]\n",
        "\n",
        "assert len(train_test_split(messages)[0]) + len(train_test_split(messages)[1]) == len(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "S_FWHEo7BCYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_V-VaTySrK6-"
      },
      "outputs": [],
      "source": [
        "# The Naive Bayes classifier\n",
        "class NaiveBayes:\n",
        "    def __init__(self, k=1) -> None:\n",
        "        # `k` is the smoothening factor\n",
        "        self._k: int = k\n",
        "        self._num_spam_messages: int = 0\n",
        "        self._num_ham_messages: int = 0\n",
        "        self._num_word_in_spam: Dict[int] = defaultdict(int)\n",
        "        self._num_word_in_ham: Dict[int] = defaultdict(int)\n",
        "        self._spam_words: Set[str] = set()\n",
        "        self._ham_words: Set[str] = set()\n",
        "        self._words: Set[str] = set()\n",
        "\n",
        "    # Iterate through the given messages and gather the necessary statistics\n",
        "    def train(self, messages: List[Message]) -> None:\n",
        "        msg: Message\n",
        "        token: str\n",
        "        for msg in messages:\n",
        "            tokens: Set[str] = tokenize(msg.text)\n",
        "            self._words.update(tokens)\n",
        "            if msg.is_spam:\n",
        "                self._num_spam_messages += 1\n",
        "                self._spam_words.update(tokens)\n",
        "                for token in tokens:\n",
        "                    self._num_word_in_spam[token] += 1\n",
        "            else:\n",
        "                self._num_ham_messages += 1\n",
        "                self._ham_words.update(tokens)\n",
        "                for token in tokens:\n",
        "                    self._num_word_in_ham[token] += 1\n",
        "\n",
        "    # Probability of `word` being spam\n",
        "    def _p_word_spam(self, word: str) -> float:\n",
        "        return (self._k + self._num_word_in_spam[word]) / ((2 * self._k) + self._num_spam_messages)\n",
        "\n",
        "    # Probability of `word` being ham\n",
        "    def _p_word_ham(self, word: str) -> float:\n",
        "        return (self._k + self._num_word_in_ham[word]) / ((2 * self._k) + self._num_ham_messages)\n",
        "\n",
        "    # Given a `text`, how likely is it spam?\n",
        "    def predict(self, text: str) -> float:\n",
        "        text_words: Set[str] = tokenize(text)\n",
        "        log_p_spam: float = 0.0\n",
        "        log_p_ham: float = 0.0\n",
        "\n",
        "        for word in self._words:\n",
        "            p_spam: float = self._p_word_spam(word)\n",
        "            p_ham: float = self._p_word_ham(word)\n",
        "            if word in text_words:\n",
        "                log_p_spam += log(p_spam)\n",
        "                log_p_ham += log(p_ham)\n",
        "            else:\n",
        "                log_p_spam += log(1 - p_spam)\n",
        "                log_p_ham += log(1 - p_ham)\n",
        "\n",
        "        p_if_spam: float = exp(log_p_spam)\n",
        "        p_if_ham: float = exp(log_p_ham)\n",
        "        return p_if_spam / (p_if_spam + p_if_ham)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Test"
      ],
      "metadata": {
        "id": "aLNTxz-QAtRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------#\n",
        "# Tests\n",
        "def test_naive_bayes():\n",
        "    messages: List[Message] = [\n",
        "        Message('Spam message', is_spam=True),\n",
        "        Message('Ham message', is_spam=False),\n",
        "        Message('Ham message about Spam', is_spam=False)]\n",
        "\n",
        "\n",
        "    nb: NaiveBayes = NaiveBayes()\n",
        "    nb.train(messages)\n",
        "\n",
        "    assert nb._num_spam_messages == 1\n",
        "    assert nb._num_ham_messages == 2\n",
        "    assert nb._spam_words == {'spam', 'message'}\n",
        "    assert nb._ham_words == {'ham', 'message', 'about', 'spam'}\n",
        "    assert nb._num_word_in_spam == {'spam': 1, 'message': 1}\n",
        "    assert nb._num_word_in_ham == {'ham': 2, 'message': 2, 'about': 1, 'spam': 1}\n",
        "    assert nb._words == {'spam', 'message', 'ham', 'about'}\n",
        "\n",
        "    # Our test message\n",
        "    text: str = 'A spam message'\n",
        "\n",
        "    # Reminder: The `_words` we iterater over are: {'spam', 'message', 'ham', 'about'}\n",
        "\n",
        "    # Calculate how spammy the `text` might be\n",
        "    p_if_spam: float = exp(sum([\n",
        "        log(     (1 + 1) / ((2 * 1) + 1)),  # `spam` (also in `text`)\n",
        "        log(     (1 + 1) / ((2 * 1) + 1)),  # `message` (also in `text`)\n",
        "        log(1 - ((1 + 0) / ((2 * 1) + 1))), # `ham` (NOT in `text`)\n",
        "        log(1 - ((1 + 0) / ((2 * 1) + 1))), # `about` (NOT in `text`)\n",
        "    ]))\n",
        "\n",
        "    # Calculate how hammy the `text` might be\n",
        "    p_if_ham: float = exp(sum([\n",
        "        log(     (1 + 1)  / ((2 * 1) + 2)),  # `spam` (also in `text`)\n",
        "        log(     (1 + 2)  / ((2 * 1) + 2)),  # `message` (also in `text`)\n",
        "        log(1 - ((1 + 2)  / ((2 * 1) + 2))), # `ham` (NOT in `text`)\n",
        "        log(1 - ((1 + 1)  / ((2 * 1) + 2))), # `about` (NOT in `text`)\n",
        "    ]))\n",
        "\n",
        "    p_spam: float = p_if_spam / (p_if_spam + p_if_ham)\n",
        "\n",
        "    assert p_spam == nb.predict(text)\n",
        "\n",
        "test_naive_bayes()"
      ],
      "metadata": {
        "id": "2Klpe2Jt3p4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6560933a-3b77-437b-be8f-ccb48255d02f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Message(text='Spam message', is_spam=True), Message(text='Ham message', is_spam=False), Message(text='Ham message about Spam', is_spam=False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note\n",
        "-"
      ],
      "metadata": {
        "id": "vZLKcuLw75V9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Oy7NPNbarK6_"
      },
      "outputs": [],
      "source": [
        "train: List[Message]\n",
        "test: List[Message]\n",
        "\n",
        "# Splitting our Enron messages into a `train` and `test` set\n",
        "train, test = train_test_split(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RINksf9grK6_",
        "outputId": "4c057061-2932-4e34-eb50-84b0228f4971",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam messages in training data: 1181\n",
            "Ham messages in training data: 2957\n",
            "Most spammy words: [('you', 106), ('your', 101), ('the', 92), ('for', 92), ('to', 85), ('re', 69), ('and', 56), ('on', 53), ('is', 46), ('get', 45), ('in', 44), ('of', 42), ('this', 39), ('software', 36), ('all', 35), ('from', 33), ('online', 33), ('with', 33), ('it', 32), ('at', 31)]\n"
          ]
        }
      ],
      "source": [
        "# Train our Naive Bayes classifier with the `train` set\n",
        "nb: NaiveBayes = NaiveBayes()\n",
        "nb.train(train)\n",
        "\n",
        "print(f'Spam messages in training data: {nb._num_spam_messages}')\n",
        "print(f'Ham messages in training data: {nb._num_ham_messages}')\n",
        "print(f'Most spammy words: {Counter(nb._num_word_in_spam).most_common(20)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "INjKA9MKrK6_",
        "outputId": "907f7434-a5b1-4402-a204-829eec5abdde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Message(text='fwd : this is is porn money - -', is_spam=True),\n",
              " Message(text='hey ,', is_spam=True),\n",
              " Message(text='date a new wild babe tonight', is_spam=True),\n",
              " Message(text='', is_spam=True),\n",
              " Message(text='re : no more injections', is_spam=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Grabbing all the spam messages from our `test` set\n",
        "spam_messages: List[Message] = [item for item in test if item.is_spam]\n",
        "spam_messages[:5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(spam_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b693i6Fk9Skt",
        "outputId": "f4a1ead4-647e-484e-9f88-b82f52a64d26"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "319"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evalutation"
      ],
      "metadata": {
        "id": "MTd977pvAziM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "BL2Zin0irK6_",
        "outputId": "3c082b59-23c6-4867-ca49-4c99d96161c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message(text='you can stay rock hard , longer', is_spam=True)\n",
            "Predicting likelihood of \"you can stay rock hard , longer\" being spam.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999797915954709"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# Using our trained Naive Bayes classifier to classify a spam message\n",
        "message= spam_messages[10]\n",
        "print(message)\n",
        "print(f'Predicting likelihood of \"{message.text}\" being spam.')\n",
        "nb.predict(message.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "SoBG9DA1rK6_",
        "outputId": "dd45e816-2f50-4708-82c8-1fc76bdb846d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Message(text='mobil beaumont', is_spam=False),\n",
              " Message(text='epgt', is_spam=False),\n",
              " Message(text='neon lesson # 5', is_spam=False),\n",
              " Message(text='point change for deals', is_spam=False),\n",
              " Message(text='new nomination', is_spam=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Grabbing all the ham messages from our `test` set\n",
        "ham_messages: List[Message] = [item for item in test if not item.is_spam]\n",
        "ham_messages[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "2SI4gqscrK7A",
        "outputId": "d2dc22da-9307-4983-b62b-a5f5bc6312b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message(text='from your clerk . its been great !', is_spam=False)\n",
            "Predicting likelihood of \"from your clerk . its been great !\" being spam.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9934977992916378"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# Using our trained Naive Bayes classifier to classify a ham message\n",
        "message = ham_messages[]\n",
        "print(message)\n",
        "print(f'Predicting likelihood of \"{message.text}\" being spam.')\n",
        "nb.predict(message.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from math import exp\n",
        "\n",
        "def predict_spam(model, test_data):\n",
        "    \"\"\"\n",
        "    Predicts spam/ham for test data based on a probability threshold of 0.5.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for message in test_data:\n",
        "        # Probability from model's prediction\n",
        "        probability = model.predict(message.text)\n",
        "        # Determine spam/ham: Spam if probability > 0.5, Ham otherwise\n",
        "        is_spam = probability > 0.5\n",
        "        predictions.append(is_spam)\n",
        "    return predictions\n",
        "\n",
        "# Assuming 'test' is a list of Message objects with `text` and `is_spam` attributes\n",
        "# Generate predictions\n",
        "predictions = predict_spam(nb, test)\n",
        "\n",
        "# Extract actual labels\n",
        "actual_labels = [message.is_spam for message in test]\n",
        "\n",
        "# Evaluate performance\n",
        "conf_matrix = confusion_matrix(actual_labels, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Classification Report: Includes F1, Recall, and Precision\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(actual_labels, predictions, target_names=[\"Ham\", \"Spam\"]))\n",
        "\n",
        "# Manually calculate metrics\n",
        "true_positive = conf_matrix[1, 1]  # Spam correctly classified as Spam\n",
        "false_positive = conf_matrix[0, 1]  # Ham incorrectly classified as Spam\n",
        "false_negative = conf_matrix[1, 0]  # Spam incorrectly classified as Ham\n",
        "true_negative = conf_matrix[0, 0]  # Ham correctly classified as Ham\n",
        "\n",
        "precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
        "recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(\"\\nManual Metrics:\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMTRP5vhCei9",
        "outputId": "1ee8d453-a793-484c-a0f9-e6601c406ad3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[694  21]\n",
            " [100 219]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.87      0.97      0.92       715\n",
            "        Spam       0.91      0.69      0.78       319\n",
            "\n",
            "    accuracy                           0.88      1034\n",
            "   macro avg       0.89      0.83      0.85      1034\n",
            "weighted avg       0.89      0.88      0.88      1034\n",
            "\n",
            "\n",
            "Manual Metrics:\n",
            "Precision: 0.91\n",
            "Recall: 0.69\n",
            "F1-Score: 0.78\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}